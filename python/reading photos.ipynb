{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95178983",
   "metadata": {},
   "source": [
    "# 1. Importing libraries & options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ffb349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ['TF_LIBRARY_PATH'] = 'C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2/nvvm/libdevice/libdevice.10.bc'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.layers import Conv2D, MaxPool2D, Dropout, \\\n",
    "Dense, Input, GlobalAveragePooling2D, AveragePooling2D\n",
    "\n",
    "import regex as re\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d1387a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf71f90a",
   "metadata": {},
   "source": [
    "# 2. Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4fd3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the name of the folders\n",
    "folders = [name[0] for i,name in enumerate(os.walk('../data/train')) if i !=0]\n",
    "#folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fb9930",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Reading images\n",
    "images_list = []\n",
    "series = 0\n",
    "for folder in folders:\n",
    "    \n",
    "    # reading the name of the animal\n",
    "    animal_class = folder[0].replace('../data/train/','').lower().replace(' ','_')\n",
    "    \n",
    "    # reading all the images\n",
    "    images = [name for name in os.walk(folder)][0][2]\n",
    "    \n",
    "    # --> looping for every image in the folder\n",
    "\n",
    "    count = 0\n",
    "    fig, ax = plt.subplots(1, 5,figsize=(10,10))\n",
    "    for i,image in enumerate(images):\n",
    "        image = Image.open(folder+'/'+image)\n",
    "        ax[i-1].imshow(image)\n",
    "        ax[i-1].set_xticks([])\n",
    "        ax[i-1].set_yticks([])\n",
    "        count += 1\n",
    "        if count == 5:\n",
    "            break\n",
    "        \n",
    "    plt.show()\n",
    "    series += 1\n",
    "    if series == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4908ddcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,  # rescale the pixel values to [0, 1]\n",
    "    #shear_range=0.2,  # apply random shearing\n",
    "    #zoom_range=0.2,  # apply random zooming\n",
    "    horizontal_flip=True  # randomly flip the images horizontally\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory='../data',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece89439",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,  # rescale the pixel values to [0, 1]\n",
    "    #shear_range=0.2,  # apply random shearing\n",
    "    #zoom_range=0.2,  # apply random zooming\n",
    "    #horizontal_flip=True  # randomly flip the images horizontally\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory='../data/train',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    directory='../data/valid',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8851f261",
   "metadata": {},
   "source": [
    "# 3. Inception network (GooleNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5644e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPool2D, Dropout, \\\n",
    "Dense, Input, GlobalAveragePooling2D, AveragePooling2D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e986ee80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_module(input_,filters_1x1,filters_3x3_pre, filters_3x3, filters_5x5_pre, filters_5x5, filters_maxpool, name = None):\n",
    "    \n",
    "    # Inception block\n",
    "    conv_1x1 = Conv2D(filters_1x1, 1, strides=(1, 1), padding='same', activation='relu', \n",
    "                      kernel_initializer=tf.keras.initializers.glorot_uniform(seed=10),\n",
    "                      bias_initializer=bias_initializer)(input_)\n",
    "    \n",
    "    conv_3x3 = Conv2D(filters_3x3_pre, 1, strides=(1, 1), padding='same', activation='relu', \n",
    "                      kernel_initializer=tf.keras.initializers.glorot_uniform(seed=10),\n",
    "                      bias_initializer=bias_initializer)(input_)\n",
    "    conv_3x3 = Conv2D(filters_3x3, 3, strides=(1, 1), padding='same', activation='relu', \n",
    "                      kernel_initializer=tf.keras.initializers.glorot_uniform(seed=10),\n",
    "                      bias_initializer=bias_initializer)(conv_3x3)\n",
    "    \n",
    "    conv_5x5 = Conv2D(filters_5x5_pre, 1, strides=(1, 1), padding='same', activation='relu',\n",
    "                      kernel_initializer=tf.keras.initializers.glorot_uniform(seed=10),\n",
    "                      bias_initializer=bias_initializer)(input_)\n",
    "    conv_5x5 = Conv2D(filters_5x5, 3, strides=(1, 1), padding='same', activation='relu',\n",
    "                      kernel_initializer=tf.keras.initializers.glorot_uniform(seed=10),\n",
    "                      bias_initializer=bias_initializer)(conv_5x5)\n",
    "    \n",
    "    max_pool = MaxPool2D(pool_size=(3, 3),strides=(1,1),padding='same')(input_)\n",
    "    max_pool = Conv2D(filters_maxpool, 1, strides=(1, 1), padding='same', activation='relu',\n",
    "                      kernel_initializer=tf.keras.initializers.glorot_uniform(seed=10),\n",
    "                      bias_initializer=bias_initializer)(max_pool)\n",
    "\n",
    "    output =  tf.keras.layers.concatenate([conv_1x1, conv_3x3, conv_5x5, max_pool], axis=3, name=name)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4d9ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializers\n",
    "bias_initializer = tf.keras.initializers.Constant(value=0.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dfe052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOOLENET ARCHITECTURE\n",
    "n_categories = 510\n",
    "\n",
    "input_layer =  Input(shape=(224,224,3))\n",
    "\n",
    "\n",
    "# B L O C K - 1\n",
    "x = Conv2D(64, (7, 7), padding='same', strides=(2, 2), activation='relu', name='conv_1_7x7', \n",
    "           kernel_initializer=tf.keras.initializers.glorot_uniform(seed=10), bias_initializer=bias_initializer)(input_layer)\n",
    "\n",
    "x = MaxPool2D((3, 3), padding='same', strides=(2, 2), name='max_pool_1_3x3')(x)\n",
    "#x = tf.nn.local_response_normalization(x, name ='norm1')\n",
    "\n",
    "\n",
    "# B L O C K - 2\n",
    "x = Conv2D(64, (1, 1), padding='same', strides=(1, 1), activation='relu', name='conv_2a_1x1', \n",
    "           kernel_initializer=tf.keras.initializers.glorot_uniform(seed=10), bias_initializer=bias_initializer)(x)\n",
    "\n",
    "x = Conv2D(192, (3, 3), padding='same', strides=(1, 1), activation='relu', name='conv_2b_3x3', \n",
    "           kernel_initializer=tf.keras.initializers.glorot_uniform(seed=10), bias_initializer=bias_initializer)(x)\n",
    "\n",
    "#x = tf.nn.local_response_normalization(x, name ='norm2')\n",
    "x = MaxPool2D((3, 3), padding='same', strides=(2, 2), name='max_pool_2_3x3')(x)\n",
    "\n",
    "\n",
    "# B L O C K - 3 (Inception modules)\n",
    "x = inception_module(x, filters_1x1=64, filters_3x3_pre=96, filters_3x3=128, filters_5x5_pre=16,\n",
    "                     filters_5x5=32, filters_maxpool=32, name='inception_3a')\n",
    "x = inception_module(x, filters_1x1=128, filters_3x3_pre=128, filters_3x3=192, filters_5x5_pre=32,\n",
    "                     filters_5x5=96, filters_maxpool=64, name='inception_3b')\n",
    "x = MaxPool2D((3, 3), padding='same', strides=(2, 2), name='max_pool_3_3x3')(x)\n",
    "\n",
    "\n",
    "# B L O C K - 4\n",
    "x = inception_module(x, filters_1x1=192, filters_3x3_pre=96, filters_3x3=208, filters_5x5_pre=16,\n",
    "                     filters_5x5=48, filters_maxpool=64, name='inception_4a')\n",
    "\n",
    "# ---> auxilliary output 1 \n",
    "x1 = AveragePooling2D((5, 5), strides=3, padding='valid')(x)\n",
    "x1 = Conv2D(128, (1, 1), padding='same', activation='relu')(x1)\n",
    "x1 = tf.keras.layers.Flatten()(x1)\n",
    "x1 = Dense(1024, activation='relu')(x1)\n",
    "x1 = Dropout(0.7)(x1)\n",
    "x1 = Dense(n_categories, activation='softmax', name='auxilliary_output_1')(x1)\n",
    "\n",
    "x = inception_module(x, filters_1x1=160, filters_3x3_pre=112, filters_3x3=224, filters_5x5_pre=24,\n",
    "                     filters_5x5=64, filters_maxpool=64, name='inception_4b')\n",
    "x = inception_module(x, filters_1x1=128, filters_3x3_pre=128, filters_3x3=256, filters_5x5_pre=24,\n",
    "                     filters_5x5=64, filters_maxpool=64, name='inception_4c')\n",
    "x = inception_module(x, filters_1x1=112, filters_3x3_pre=144, filters_3x3=288, filters_5x5_pre=32,\n",
    "                     filters_5x5=64, filters_maxpool=64, name='inception_4d')\n",
    "\n",
    "# ---> auxilliary output 2\n",
    "x2 = AveragePooling2D((5, 5), strides=3, padding='valid')(x)\n",
    "x2 = Conv2D(128, (1, 1), padding='same', activation='relu')(x2)\n",
    "x2 = tf.keras.layers.Flatten()(x2)\n",
    "x2 = Dense(1024, activation='relu')(x2)\n",
    "x2 = Dropout(0.7)(x2)\n",
    "x2 = Dense(n_categories, activation='softmax', name='auxilliary_output_2')(x2)\n",
    "\n",
    "x = inception_module(x, filters_1x1=256, filters_3x3_pre=160, filters_3x3=320, filters_5x5_pre=32,\n",
    "                     filters_5x5=128, filters_maxpool=128, name='inception_4e')\n",
    "x = MaxPool2D((3, 3), padding='same', strides=(2, 2), name='max_pool_4_3x3')(x)\n",
    "\n",
    "\n",
    "# B L O C K - 5\n",
    "x = inception_module(x, filters_1x1=256, filters_3x3_pre=160, filters_3x3=320, filters_5x5_pre=32,\n",
    "                     filters_5x5=128, filters_maxpool=128, name='inception_5a')\n",
    "x = inception_module(x, filters_1x1=384, filters_3x3_pre=192, filters_3x3=384, filters_5x5_pre=48,\n",
    "                     filters_5x5=128, filters_maxpool=128, name='inception_5b')\n",
    "x = GlobalAveragePooling2D(name='avg_pool_5_3x3')(x)\n",
    "x = Dropout(0.4)(x)\n",
    "x = Dense(n_categories, activation='softmax', name='output')(x)\n",
    "\n",
    "model =  tf.keras.Model(input_layer, [x, x1, x2], name='inception_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef33eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f257222",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"initial_lrate = 0.01\n",
    "\n",
    "def decay(epoch, steps=100):\n",
    "    initial_lrate = 0.01\n",
    "    drop = 0.96\n",
    "    epochs_drop = 8\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate\n",
    "\n",
    "sgd = tf.keras.optimizers.SGD(learning_rate=initial_lrate, momentum=0.9, nesterov=False)\n",
    "lr_sc =  tf.keras.callbacks.LearningRateScheduler(decay, verbose=1)\n",
    "model.compile(loss=['categorical_crossentropy', 'categorical_crossentropy', 'categorical_crossentropy'], loss_weights=[1, 0.3, 0.3], optimizer=sgd, metrics=['accuracy'])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73f354c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.fit(train_generator, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea872cd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a9d891",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Include the epoch in the file name (uses `str.format`)\n",
    "checkpoint_path = \"train_cp/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "epotch_size = 2561\n",
    "frequency = 5\n",
    "\n",
    "# Create a callback that saves the model's weights every 5 epochs\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path, \n",
    "    verbose=1, \n",
    "    save_weights_only=True,\n",
    "    save_freq=epotch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18d47e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adam = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "#model.compile(loss=['categorical_crossentropy', 'categorical_crossentropy', 'categorical_crossentropy'],\n",
    "#              loss_weights=[1, 0.3, 0.3], optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "sgd = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=False)\n",
    "model.compile(loss=['categorical_crossentropy', 'categorical_crossentropy', 'categorical_crossentropy'],\n",
    "              loss_weights=[1, 0.3, 0.3], optimizer=sgd, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b987f39",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_generator, epochs=50, validation_data = val_generator, callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440cdd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584d2dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0608eb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## KEEP TRAINING\n",
    "\n",
    "# Load the previously saved weights\n",
    "model.load_weights(latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c07afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_generator, epochs=20, validation_data = val_generator, callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272eaad7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu",
   "language": "python",
   "name": "tfgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
